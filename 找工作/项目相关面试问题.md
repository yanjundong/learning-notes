# bzk项目

## 项目介绍

​		本项目是基于OGC标准，设计了一款高效、可扩展的地理信息系统，实现了WMS、WFS、WMTS、打包下载等服务功能。同时为了实现对数据访问的控制，也支持服务的发布和管理。该项目包含了Web服务模块、应用服务模块、地图数据存储管理模块、服务管理模块等多个模块。我主要负责的是地图数据存储管理模块。

如果再细分的话，我是负责地图数据中的WMTS数据管理。下面就以WMTS服务为例介绍这个项目：

WMTS更加类似于我们平时看到的地图，在整体上看 他就像是一张图片一样，可以移动、放大、缩小等，但真实上它是由很多张的图片拼成的，这些图片被称为瓦片，大小为256 x 256或其他。如果把全球看作一张地图的话，在最顶层是只有一张图片，在放大一个level后，一张图片会变成4张，此时地图的精度变高 能够显示的内容也会更多。当然在更高层次后，图片的数量也会呈指数级增长，这就增大了数据存储和读取的难度。因为level越高图片数量越多的现象所以这张地图在整体上被称为金字塔。

在我们的实验中会包含多个金字塔，通常是一个县或者一个村落。

在这个项目中我的主要工作也就是管理这些数据，支持金字塔的入库和出库、显示浏览等，并编写restful接口供其他模块调用。

## 主要工作

![](https://gitee.com/yanjundong97/picBed/raw/master/images/image-20210221141859378.png)

**1、索引怎么存？**

每一张瓦片都可以用 level、row、col来唯一确定，这里的3个坐标是相对全球的金字塔的坐标。

在存储时，为了更好的利用hdfs的块缓存，需要把空间邻近的瓦片尽可能放在同一个block块中。四叉树编码在一定程度能够满足这一要求。

我是把金字塔的uid拼上瓦片的四叉树编码作为一张瓦片的id，同时还会存储该瓦片在hdfs中存储的文件名称、偏移量、是否逻辑删除等信息。

**2、为什么采用这样的存储模型？**

- 地图瓦片数据具有单个文件小、总体容量大的特点，而HDFS不能高效的处理小文件，因此不能直接放入HDFS中，否则会占满NameNode；
- 地图瓦片数据的访问具有空间邻近的特点，也就是说在一个屏幕上会显示一个区域的地图，而不是某一行的瓦片数据，如果按照一行行的存储瓦片数据，会造成大量的寻址时间。HDFS存档文件和HAR文件都不能满足这样的需求。
- 在HBase中存储了单个瓦片的索引，之所以没有把瓦片数据作为一列来存储，是为了提高金字塔的出库速率。

## 遇到的难点

**整个项目的难点：**

1. 对地理相关知识的不了解，EPSG:4326、EPSG:3857等，WFS、WMTS、WMS等；

2. 瓦片的存储选择怎样的存储模型，为什么；

   HDFS提供了HAR、SequenceFile、MapFile等方案解决海量小文件存储，但是都不能满足随机访问瓦片的要求，因此自定义实现针对瓦片访问特点的打包、索引和缓存策略，充分提高瓦片在高并发情况下的读写性能。

3. Redis的缓存一致性问题

**负载均衡层：**

一开始开发都是以单节点进行开发，到了最终联调部署的时候成了1台nginx反向代理服务器+8台web服务节点的微服务架构，在部署过程中出现了一些问题：

1、为什么要使用微服务架构模式：

- 每个模块需要的服务器资源不同，比如WMTS，WFS等为IO密集型，WMS为CPU密集型，拆分后可以更好的利用服务器资源。
- 业务逻辑比较清晰简单。

2、调用某个OGC服务，需要调用多个子服务，这些子服务前后之间具有依赖关系，需要在同一个节点上被调用；而采用轮询的负载均衡方式会导致各个子服务在不同节点之间被调用，最终导致该OGC服务调用失败。

解决办法：

- 采用IP hash负载均衡，但是默认的nginx的ip hash算法是用ip的前3个字节进行hash运算，这在一个局域网内是无意义的，因此采用lua脚本实现了一个自定义的ip hash算法。
- 暂时将解耦的子服务耦合。

**Redis缓存服务：**

对于一些需要实时计算和渲染的OGC服务如WMS，速度很慢，需要使用redis进行缓存来提升速度。但是如果在服务端进行缓存添加，对代码的侵入很大，因此决定在反向代理层使用lua脚本添加redis缓存。

优点：对后台服务的代码侵入少、减少了后台服务器的请求量；

缺点：存在缓存一致问题、可能增大服务均衡服务器的压力；

**服务管理模块：**

修改配置不能马上生效

解决办法：

- 服务器每隔一段时间去主动询问；
- zookeeper通知修改配置，收到通知后去获取新的配置；

**数据管理模块：**

瓦片的存储模型设计，为什么？

## 还可以优化的地方

- 由于时间上的问题，目前使用的是jar的群起方式部署，后面可以改成Docker化部署，并使用k8s进行容器的编排等。
- 

## 常见问题

**1、HDFS的优缺点？**

**2、HBase的优缺点？**

**3、HDFS的日志机制**







































